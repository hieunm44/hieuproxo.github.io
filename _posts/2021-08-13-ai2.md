---
layout: post
title: "Review sách: Pattern Recognition And Machine Learning"
date: 2021-08-13 23:26:24 +0700
category: AI
---

[Pattern Recognition And Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

# Chapter 1. Introduction
## 1.1. Polynomial Curve Fitting
Phần này giới thiệu bài toán _regression_ cơ bản, là fit  $N$ observations $\mathbf{x}=(x_1,\dots,x_N)^T$ với $N$ corresponding observations $\mathbf{t}=(t_1,\dots,t_N)^T$. Có hai cách để giải bài toán này:
1. Dùng một polynomial function (lúc này bài toán được gọi là _linear regression_)
2. Dùng *Probability Theory* (1.2) kết hợp với *Decision Theory* (1.5)

Phần này trình bày cách thứ nhất, cùng với đó là hiện tượng *over-fitting* và cách khắc phục dùng *regularlization*.

## 1.2. Probability Theory
Phần đầu của 1.2 giới thiệu những khái niệm cơ bản về lí thuyết xác suất, bao gồm *random variable*, *joint probability*, *marginal probability*, *conditional probability*, *sum rule*, *product rule*, *Bayes' theorem*, *prior probability*, và *posterior probability*.

### 1.2.1 Probability densities
Phần này giới thiệu các khái niệm về xác suất cho biến liên tục, bao gồm *probability density*, *cummulative distribution function*, *joint probability density*, *sum rule* và *product rule* cho biến liên tục.

### 1.2.2 Expectations and covariances
Phần này giới thiệu các khái niệm *expectation*, *variance* cho biến rời rạc và biến liên tục, cùng với đó là khái niệm *covariance* cho hai biến và hai vector.

### 1.2.3 Bayesian probabilities
Các xác suất vẫn xét từ trước đến nay được gọi là *classical* hay *frequentist interpretation of probability*. Phần này giới thiệu *Bayesian view*, trong đó xác suất thể hiện *quantification of uncertainty*.

Trong bài toán polynomial curve fitting, đầu tiên ta đưa ra assumption về  $\mathbf{w}$ thông qua $p(\mathbf{w})$, sau đó quan sát $\mathcal{D}={t_1,\dots,t_N}$ thông qua $p(\mathcal{D}\|\mathbf{w})$, sau đó dùng định lí Bayes để xác định $p(\mathbf{w}\|\mathcal{D})$. Phần này cũng giải thích sự khác nhau của $p(\mathbf{w}\|\mathcal{D})$ trong frequentist view và Bayesian view, sau đó giới thiệu một frequentist estimator phổ biến là *maximum likelihood*.

### 1.2.4 The Gaussian distribution
Phần này giới thiệu *Gaussian distribution* cho trường hợp biến đơn và cho vector, sau đó là dùng maximum likelihood để ước lượng các tham số $\mu$ và $\sigma^2$ cho Gaussian distribution sau khi có tập các observations $\mathbf{x}=(x_1,\dots,x_N)^T$, với giả sử các điểm dữ liệu là *independant and identically distributed (i.i.d)*. Phần này cũng trình bày một hạn chế của maximumlikelihood là nó *systematically underestimates the variance of the distribution* (hiện tượng này được gọi là *bias*) và cách khắc phục, sau đó chỉ ra hiện tượng bias này có gốc rễ ở over-fitting trong polynomial curve fitting.

### 1.2.5 Curve fitting re-visited
Phần này áp dụng maximum likelihood vào bài toán polynomial curve fitting, giới thiệu *predictive distribution*. Sau đó phần này giới thiệu một *more Bayesian approach* là *maximum posterior*, cuối cùng đưa ra kết luận quan trọng:
* Maximum likelihood tương đương với linear regression bình thường
* Maximum posterior tương đương với linear regression dùng regularization

### 1.2.6 Bayesian curve fitting
Mặc dù đã xét prior distribution, đây vẫn chỉ là dự đoán một điểm $\mathbf{w}$, chưa phải là *fully Bayesian approach*. Trong fully Bayesian approach, ta cần dùng sum và product rules, trong đó cần lấy tích phân trên tất cả các giá trị của $\mathbf{w}$. *Such marginalizations lie at the heart of Bayesian methods for pattern recognition*.

Phần này áp dụng sum và product rules vào predictive distribution.

## 1.3. Model Selection
Phần này trình bày kĩ thuật lựa chọn mô hình dùng *cross-validation*.

## 1.4. The Curse of Dimensionality
Phần này trình bày hiện tượng dữ liệu không có tính tổng quát khi số chiều dữ liệu tăng, được gọi là *The curse of dimensionality*. Tuy nhiên, phần này sau đó cũng giải thích vẫn có thể xây dựng mô hình hiệu quả trên không gian nhiều chiều.

## 1.5. Decision Theory
Phần đầu của 1.5 chỉ ra kết hợp Probability theory và Decision Theory sẽ giúp đưa ra optimal decisions trong các trường hợp liên quan đến unvertainty như trong pattern recognition, sau đó đưa ra ví dụ về chẩn đoán y tế bằng ảnh X-ray.

### 1.5.1 Minimizing the classification rate
Phần này trình bày về tối thiểu hóa xác suất phân loại lỗi cho trường hợp $2$ lớp và $K$ lớp.

### 1.5.2 Minimizing the expected loss
Phần này trình bày loss function đo mất mát khi đưa ra decisions, giới thiệu *loss matrix* cho ví dụ chẩn đoán ung thư, trong đó trừng phạt các trường hợp dự đoán sai khác nhau theo mức độ nghiêm trọng.

### 1.5.3 The reject option
Phần này đưa ra trường hợp mà có thể không đưa ra decisions, ví dụ dùng máy để phân loại trước các ảnh X-ray với độ chắc chắn cao và để lại ảnh khó cho bác sĩ, dựa vào một ngưỡng $\theta$.

### 1.5.4 Inference and decision
Phần này tổng hợp ba phương pháp để giải decision problem, ưu nhược điểm từng phương pháp, và các lí do cần xác định posterior $p(\mathcal{C}_k\|\mathbf{x})$.

### 1.5.5 Loss function for regression
Phần này trình bày áp dụng decision theory vào bài toán regression, sau đó tổng hợp ba phương pháp để giải bài toán regression.

## 1.6. Information Theory
Phần đầu 1.6 giới thiệu khái niệm *entropy* cho biến rời rạc và biến liên tục, giới thiệu *differential entropy*, sau đó giải bài toán maximize the differential entropy, kết quả tìm được phân phối Gaussian,, cuối cùng giới thiệu *conditional entropy*.

### 1.6.1 Relative entropy and mutual information
Phần này giới thiệu *relative entropy* hay *Kullback-Leibler divergence*, *convex function*, *Jensen's inequality*, và *mutual information*.

# Chapter 2. Probability Distributions
## 2.1. Binary Values
Phần đầu 2.1 trình bày *Bernoulli distribution* $Bern(x\|\mu)$ cho biến ngẫu nhiên nhị phân $x$ và ước lượng tham số $\mu$ bằng maximum likelihood. Sau đó phần này trình bày *Binomial distribution* $Bin(m\|N,\mu)$ khi xem xét $m$ lần xảy ra observation $x=1$ trong $N$ lần thử.

### 2.1.1 The beta distribution
Phần này giới thiệu tính chất *conjugacy* khí xây dựng prior $p(\mu)$ và xác định posterior để tránh overfitting trong maximum likelihood, sau đó giới thiệu chọn *beta distribution* làm prior.

## 2.2. Multinomial Variables
Phần đầu 2.2 giới thiệu các biểu diễn một biến nhị phân $\mathbf{x}$ có $K$ states bằng one-hot vector, với xác suất $x_k=1$ là $\mu$, đưa ra phân phối của $\mathbf{x}$ và xác định $\mu_k$ bằng maximum likelihood, sau đó giới thiệu *multinomial distribution*.

### 2.2.1 The Dirichlet distribution
Phần này giới thiệu conjugate prior cho multinomial distribution là *Dirichlet distribution*

## 2.3. The Gaussian Distribution
Phần đầu 2.3 đưa lại công thức Gaussian distribution cho trường hợp biến đơn và cho vector, giới thiệu *central limit theorem*, trình bày geometrical form của Gaussian distribution, cuối cùng trình bày hai hạn chế của multivariate Gaussian distribution và cách khắc phục (trong đó có nhắc đến *mixtures of Gaussian*).

### 2.3.1 Conditional Gaussian distribution
Phần này giới thiệu một tính chất quan trọng của multivariate Gaussian distribution: Nếu có hai tập các biến là jointly Gaussian, thì conditional distribution của một tập conditioned trên tập còn lại cũng là Gaussian và marginal distribution cũng là Gaussian.

Phần này sau đó trình bày *conditional Gaussian distribution* thông qua việc chia vector $\mathbf{x}$ thành hai vector $\mathbf{x_a}$ và $\mathbf{x_b}$. 

### 2.3.2 Marginal Gaussian distribution
Phần này trình bày về *marginal Gaussian distribution*, cuối cùng tổng kết về *paritioned Gaussians*, bao gồm conditional và marginal Gaussian distribution.

### 2.3.3 Bayes' theorem for Gaussian variables
Ở 2.3.1 và 2.3.2, ta đã xét một Gaussian $p(\mathbf{x})$ trong đó ta chia vector $\mathbf{x}$ thành hai vector con $\mathbf{x}=(\mathbf{x_a}, \mathbf{x_b})$ và tìm conditional distribution $p(\mathbf{x_a}, \mathbf{x_b})$ và marginal distribution $\mathbf{x_a}\|\mathbf{x_b}$, trong đó $\mathbf{x_a}\|\mathbf{x_b}$ có mean là một hàm tuyến tính theo $\mathbf{x_b}$. Ở phần này giả sử ta cí một Gaussian marginal distribution $p(\mathbf{x})$ và một Gaussian condition distrubtion $p(\mathbf{y}\|\mathbf{x})$, trong đó $p(\mathbf{y}\|\mathbf{x})$ có mean là một hàm tuyến tính theo $\mathbf{x}$. Ta cần xác định marginal distribution $p(\mathbf{y})$ và conditional distribution $p(\mathbf{x}\|\mathbf{y})$.

Phần này lần lượt trình bày tìm biểu diễn cho $p(\mathbf{x}, \mathbf{y})$, $p(\mathbf{y})$, $p(\mathbf{x}\|\mathbf{y})$, cuối cùng tổng kết về margianl và conditional Gaussians.

### 2.3.4 Maximum likelihood for the Gaussian
Phần này trình bày tìm $\mathbf{\mu}$ và $\mathbf{\Sigma}$ cho multivariate Gaussian distribution dùng maximum likelihood và giới thiệu tính chất *sufficient statistics*.

### 2.3.5 Sequential estimation
Phần này trình bày về ước lượng tham số trong Gaussians cho chuỗi các data points dùng thuật toán *Robbins-Monro*.

### 2.3.6 Bayesian inference for the Gaussian
Phần này xây dựng *Bayesian treatment* với prior distributions theo $\mathbf{\mu}$ và $\mathbf{\Sigma}$, chỉ xét biến Gaussian đơn và xét các trường hợp: Đã biết $\sigma^2$ và cần infer ra $\mu$ khi biết $N$ observations; Đã biết $\mu$ và cần infer ra $\sigma^2$; Chưa biết cả $\mu$ và $\sigma^2$ và cần tìm conjugate prior, sau đó giới thiệu *normal-gamma* hay *Gaussian-gamma distribution*.

### 2.3.7 Student's t-distribution
Phần này giới thiệu *Student's t-distribution*, là cộng dồn vô hạn các Gaussian distributions có cùng mean nhưng khác precisions, được gọi là *infinite mixture of Gaussians*, sau đó giới thiệu tính chất quan trọng của t-distribution là *robustness*, nghĩa là ít nhạy cảm với nhiễu hơn so với Gaussian. Robustness cũng là một tính chất quan trọng cho bài toán regression.

### 2.3.8 Periodic variables
Phần này giới thiệu biến tuần hoàn được biểu diễn qua góc $\theta$ , giới thiệu *a periodic generalization of the Gaussian*, được gọi là *von Mises distribution*, và maximum likelihood cho $p(\theta)$.

### 2.3.9 Mixtures of Gaussians
Phần này chỉ r, dùng một lượng đủ lớn các Gaussians và điểu chỉnh các means, covariances và các coefficients trong linear combination, hầu hết các continuous density có thể được xấp xỉ với độ chính xác tùy ý. Phần này sau đó giới thiệu *mixture of Gaussians* và xác định các tham số của mixture bằng maximum likelihood.

## 2.4. The Exponential Family
Phần đầu 2.4 chỉ ra các phân phối từ trước đến nay (trừ Gaussian mixture) đều là các trường hợp cụ thể cuar *exponential family*, sau đó đưa ra công thức tổng quát của exponential family và xét với một số phân phối đã biết, bao gồm Bernoulli distribution, multinomial distribution, và Gaussian distribution.

### 2.4.1 Maximum likelihood and sufficient statistics
Phần này xét bài toán xác định tham số $\eta$ trong công thức tổng quát của exponential family dùng maximum likelihood và làm rõ tính chất *sufficient statistics*.

### 2.4.2 Conjugate priors
Phần này trình bày conjugate prior cho exponential family.

### 2.4.3 Noninformative priors
Trong nhiều trường hợp, chúng ta biết rất ít về dạng của distribution cần tìm, do đó cần xây dựng một dạng của prior distribution, được gọi là *noninformative prior*, mà có càng ít ảnh hưởng lên posterior distribution càng tốt.

Phần này lần lượt trình bày hai ví dụ của noninformative prior, với các tham số tương ứng được gọi là *location parameter* (ví dụ là $\mu$ trong Gaussian) và *scale parameter* (ví dụ là $\sigma$ trong Gaussian); các tính chất tương ứng là *translation invariance* và *scale invariance*.

## 2.5. Nonparametric Methods
Từ trước đến nay đã xét các phân phối phụ thuộc vào các parameters mà cần được xác định từ data set, đây được gọi là *parametric approach*. Phương pháp này có hạn chế là nếu density được chọn là poor model của phân phối tạo ra dữ liệu, thì kết quả là poor predictive performance. Phần này sẽ trình bày các *nonparametric approaches*.

Phần đầu 2.5 giới thiệu *histogram method* và chỉ ra lợi thế và hạn chế (trong đó có một hạn chế là ví dụ của The Curse of Dimensionality) của nó.

### 2.5.1 Kernel density estimators
Đầu tiên phần này xét bài toán tổng quát: Giả sử observation được lấy ra từ một unknown probability density $p(\mathbf{x})$, và cần estimate $p(\mathbf{x})$. Xét một miền nhỏ $\mathcal{R}$ chứa $\mathbf{x}$, probability mass của miền này là $P=\int_\mathcal{R} p(\mathbf{x})d\mathbf{x}$,. Giả sử thu thập được một data set gồm $N$ observations được lấy từ $p(\mathbf{x})$, ta có mỗi data point sẽ có xác suất rơi vào $\mathcal{R}$ là $P$, gọi $K$ là tổng số points rơi vào $\mathcal{R}$, đặt $V$ là volume của $\mathcal{R}$. Với giả sử $N$ đủ lớn, và $\mathcal{R}$ đủ nhỏ để $p(\mathbf{x})$ gần như là hằng số trên $\mathcal{R}$ (hai giả sử này mâu thuẫn), ta có kết quả ước lượng $p(\mathbf{x})$ là $p(\mathbf{x})=\frac{K}{NV}$. Ta có hai cách khai thác kết quả này:
1. Cố định $K$, tìm $V$ từ data $\rightarrow$ K-nearest neighbor
2. Cố định $V$, tìm $K$ từ data $\rightarrow$ Kernal approach 

Phần này sau đó trình bày *kernel method*, ý tưởng là lấy $\mathcal{R}$ là một hypercube nhỏ centered on the point $\mathbf{x}$ mà tại đó cần xác định $p(\mathbf{x})$, sau đó đếm $K$ points rơi vào $\mathcal{R}$ và tính $p(\mathbf{x})$.

### 2.5.2 Nearest-neighbor methods
Phần này chỉ ra một khó khăn của kernel approach là cần xác định curberside $h$ và khắc phục bằng *nearest-neighbor method*. Ý tưởng chính là xét một sphere nhỏ centered on the point $\mathbf{x}$ mà tại đó cần xác định $p(\mathbf{x})$ và cho phép bán kính sphere tăng đến khi nó chứa $K$ data points. Phần này cũng trình bày áp dụng K-nearest neighbor vào bài toán phân loại, nêu lên hạn chế của nó, cuối cùng chỉ ra hạn chế của các simple parametric models.

_Bài viết này vẫn tiếp tục được cập nhật._
